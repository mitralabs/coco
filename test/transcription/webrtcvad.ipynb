{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webrtcvad -q\n",
    "!pip install setuptools -q\n",
    "!pip install pydub -q\n",
    "#!pip install audioop-lts -q #only needed for python v3.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webrtcvad\n",
    "import wave\n",
    "\n",
    "vad = webrtcvad.Vad(1)\n",
    "\n",
    "def vad(audio_file,aggressiveness =1, frame_duration=30):\n",
    "    vad = webrtcvad.Vad(aggressiveness)  # 0 to 3, while 3 is the most aggressive\n",
    "    \n",
    "    with wave.open(audio_file, 'rb') as wf:\n",
    "        sample_rate = wf.getframerate()\n",
    "        \n",
    "        # Calculate frame size (must be 10, 20, or 30 ms for WebRTC VAD)\n",
    "        if frame_duration not in [10, 20, 30]:\n",
    "            frame_duration = 30\n",
    "            \n",
    "        frame_size = int(sample_rate * frame_duration / 1000)\n",
    "        \n",
    "        segments = []\n",
    "\n",
    "        while True:\n",
    "            frame = wf.readframes(frame_size)\n",
    "            if not frame:\n",
    "                break\n",
    "                \n",
    "            # Ensure we have enough samples for a complete frame\n",
    "            if len(frame) < frame_size * 2:  # *2 because 16-bit samples\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                is_speech = vad.is_speech(frame, sample_rate)\n",
    "                segments.append({\n",
    "                    'start': wf.tell() / sample_rate - frame_duration / 1000,\n",
    "                    'end': wf.tell() / sample_rate,\n",
    "                    'is_speech': is_speech\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame: {e}\")\n",
    "                continue\n",
    "\n",
    "        return segments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAD in one snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .wav files into a list\n",
    "import os\n",
    "path = \"../_data/audio_files/audio_files/\"\n",
    "audio_files = [f for f in os.listdir(path) if f.endswith('.wav')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "# Show audio file as playable widget\n",
    "audio_file = path + audio_files[i]\n",
    "ipd.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_same_segments(segments):\n",
    "    previous_is_speech = None\n",
    "    new_segments = []\n",
    "    for segment in segments:\n",
    "        if segment['is_speech'] != previous_is_speech:\n",
    "            if previous_is_speech is not None:\n",
    "                new_segments.append({\n",
    "                    'start': start,\n",
    "                    'end': segment['start'],\n",
    "                    'is_speech': previous_is_speech\n",
    "                })\n",
    "            start = segment['start']\n",
    "        previous_is_speech = segment['is_speech']\n",
    "    # Append last segment\n",
    "    new_segments.append({\n",
    "        'start': start,\n",
    "        'end': segment['end'],\n",
    "        'is_speech': previous_is_speech\n",
    "    })\n",
    "    return new_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356_142_25-02-28_10-29-30_middle.wav\n",
      "\n",
      "[{'start': 0.0, 'end': 0.15, 'is_speech': True}, {'start': 0.15, 'end': 0.8099999999999999, 'is_speech': False}, {'start': 0.8099999999999999, 'end': 3.9000000000000004, 'is_speech': True}, {'start': 3.9000000000000004, 'end': 3.93, 'is_speech': False}, {'start': 3.93, 'end': 4.17, 'is_speech': True}, {'start': 4.17, 'end': 5.67, 'is_speech': False}, {'start': 5.67, 'end': 6.39, 'is_speech': True}, {'start': 6.39, 'end': 6.6, 'is_speech': False}, {'start': 6.6, 'end': 8.940000000000001, 'is_speech': True}, {'start': 8.940000000000001, 'end': 9.3, 'is_speech': False}, {'start': 9.3, 'end': 9.99, 'is_speech': True}]\n"
     ]
    }
   ],
   "source": [
    "# Process one audio file\n",
    "segments = vad(path + audio_files[i])\n",
    "\n",
    "print(audio_files[i])\n",
    "print()\n",
    "\n",
    "print(append_same_segments(segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAD in multiple snippets. Plus (longer) silence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamp(filename: str) -> datetime.datetime:\n",
    "    # Expecting format: session_index_yy-mm-dd_hh-mm-ss_suffix.wav\n",
    "    pattern = r'_(\\d{2}-\\d{2}-\\d{2})_(\\d{2}-\\d{2}-\\d{2})_'\n",
    "    m = re.search(pattern, filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Filename {filename} does not match the required format\")\n",
    "    date_str = m.group(1)\n",
    "    time_str = m.group(2)\n",
    "    return datetime.datetime.strptime(date_str + \"_\" + time_str, \"%y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='../_data/audio_files/audio_files/full_audio.wav'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load .wav files into a list\n",
    "import os\n",
    "path = \"../_data/audio_files/audio_files/\"\n",
    "audio_files = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
    "\n",
    "# Remove files with 0 bytes\n",
    "audio_files = [f for f in audio_files if os.path.getsize(path + f) > 0]\n",
    "\n",
    "try:\n",
    "    file_list_sorted = sorted(audio_files, key=parse_timestamp)\n",
    "\n",
    "    # Append all the audio files into a single AudioSegment\n",
    "    full_audio = AudioSegment.empty()\n",
    "    for audio_file in file_list_sorted:\n",
    "        full_audio += AudioSegment.from_wav(path + audio_file)\n",
    "\n",
    "    # Export the full audio to a single file\n",
    "    full_audio.export(f\"{path}full_audio.wav\", format=\"wav\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing files: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_silence_threshold(segments, silence_threshold=0.5):\n",
    "    new_segments = []\n",
    "    for segment in segments:\n",
    "        #Speech stays speech\n",
    "        if segment['is_speech']:\n",
    "            new_segments.append(segment)\n",
    "        #Silence below threshold is not kept as silence\n",
    "        elif not segment['is_speech'] and segment['end'] - segment['start'] < silence_threshold:\n",
    "            new_segments.append({\n",
    "                'start': segment['start'],\n",
    "                'end': segment['end'],\n",
    "                'is_speech': True\n",
    "            })\n",
    "        #Silence above threshold is kept as silence    \n",
    "        elif not segment['is_speech'] and segment['end'] - segment['start'] > silence_threshold:\n",
    "            new_segments.append(segment)\n",
    "    \n",
    "    new_segments = append_same_segments(new_segments)\n",
    "    return new_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_segments(segments):\n",
    "    for segment in segments:\n",
    "        # Format timestamps to minutes and full seconds rounded \n",
    "        start = datetime.timedelta(seconds=round(segment['start']))\n",
    "        end = datetime.timedelta(seconds=round(segment['end']))\n",
    "        print(f\"Start: {start} - End: {end} - Speech: {segment['is_speech']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 0:00:00 - End: 0:28:36 - Speech: True\n",
      "Start: 0:28:36 - End: 0:28:47 - Speech: False\n",
      "Start: 0:28:47 - End: 0:53:57 - Speech: True\n",
      "Start: 0:53:57 - End: 0:54:25 - Speech: False\n",
      "Start: 0:54:25 - End: 0:54:25 - Speech: True\n",
      "Start: 0:54:25 - End: 0:54:44 - Speech: False\n",
      "Start: 0:54:44 - End: 0:55:14 - Speech: True\n",
      "Start: 0:55:14 - End: 0:55:36 - Speech: False\n",
      "Start: 0:55:36 - End: 1:22:56 - Speech: True\n",
      "Start: 1:22:56 - End: 1:23:08 - Speech: False\n"
     ]
    }
   ],
   "source": [
    "path = \"../_data/audio_files/audio_files/\"\n",
    "\n",
    "# Get segments for the full audio\n",
    "audio_file = f\"{path}full_audio.wav\"\n",
    "\n",
    "original_segments = vad(audio_file)\n",
    "\n",
    "appended_segments = append_same_segments(original_segments)\n",
    "\n",
    "segments_after_threshold = apply_silence_threshold(appended_segments, 10)\n",
    "\n",
    "pretty_print_segments(segments_after_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight from Claude**\n",
    "Silence duration threshold:\n",
    "- 300-700ms to identify a true pause between speakers\n",
    "- 150-300ms for within-speaker pauses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_speech_silence_distribution(appended_segments, thresholds):\n",
    "    \"\"\"\n",
    "    Plot speech vs silence distribution as a stacked bar chart for different thresholds.\n",
    "    \n",
    "    Args:\n",
    "        appended_segments: The segments to analyze\n",
    "        thresholds: List of threshold values to test\n",
    "    \"\"\"\n",
    "    # Lists to store durations for each threshold\n",
    "    silence_durations = []\n",
    "    speech_durations = []\n",
    "    \n",
    "    # Calculate durations for each threshold\n",
    "    for threshold in thresholds:\n",
    "        segments = apply_silence_threshold(appended_segments, threshold)\n",
    "        \n",
    "        # Calculate total duration of silence and speech\n",
    "        silence_duration = sum(segment['end'] - segment['start'] \n",
    "                              for segment in segments if not segment['is_speech'])\n",
    "        speech_duration = sum(segment['end'] - segment['start'] \n",
    "                             for segment in segments if segment['is_speech'])\n",
    "        \n",
    "        silence_durations.append(silence_duration)\n",
    "        speech_durations.append(speech_duration)\n",
    "    \n",
    "    # Convert to minutes if the values are very large\n",
    "    convert_to_minutes = max(max(silence_durations), max(speech_durations)) > 1000\n",
    "    if convert_to_minutes:\n",
    "        silence_durations = [d / 60 for d in silence_durations]\n",
    "        speech_durations = [d / 60 for d in speech_durations]\n",
    "        y_label = 'Duration (minutes)'\n",
    "    else:\n",
    "        y_label = 'Duration (seconds)'\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Use positions 0, 1, 2, etc. for the bars\n",
    "    positions = range(len(thresholds))\n",
    "    \n",
    "    # Create the stacked bars\n",
    "    ax.bar(positions, silence_durations, label='Silence', color='lightgray')\n",
    "    ax.bar(positions, speech_durations, bottom=silence_durations, \n",
    "           label='Speech', color='steelblue')\n",
    "    \n",
    "    # Add labels and styling\n",
    "    ax.set_xlabel('Silence Threshold')\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title('Distribution of Speech vs Silence by Threshold')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    # Add text showing percentages on each bar\n",
    "    total_durations = np.array(silence_durations) + np.array(speech_durations)\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        # Calculate percentages\n",
    "        silence_pct = silence_durations[i] / total_durations[i] * 100\n",
    "        speech_pct = speech_durations[i] / total_durations[i] * 100\n",
    "        \n",
    "        # Add percentage labels\n",
    "        ax.text(i, silence_durations[i]/2, f\"{silence_pct:.1f}%\", \n",
    "                ha='center', va='center', color='black')\n",
    "        ax.text(i, silence_durations[i] + speech_durations[i]/2, f\"{speech_pct:.1f}%\", \n",
    "                ha='center', va='center', color='white')\n",
    "    \n",
    "    # Set x-ticks to positions and label them with threshold values\n",
    "    ax.set_xticks(positions)\n",
    "    ax.set_xticklabels(thresholds)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example thresholds to test\n",
    "thresholds = [0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4]\n",
    "\n",
    "# Run the function\n",
    "plot_speech_silence_distribution(appended_segments, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_speech_silence_distribution(appended_segments, thresholds):\n",
    "    \"\"\"\n",
    "    Plot average duration of speech and silence segments with min/max whiskers.\n",
    "    \n",
    "    Args:\n",
    "        appended_segments: The segments to analyze\n",
    "        thresholds: List of threshold values to test\n",
    "    \"\"\"\n",
    "    # Data structures to store statistics\n",
    "    silence_avg_durations = []\n",
    "    speech_avg_durations = []\n",
    "    silence_min_durations = []\n",
    "    silence_max_durations = []\n",
    "    speech_min_durations = []\n",
    "    speech_max_durations = []\n",
    "    \n",
    "    # Calculate statistics for each threshold\n",
    "    for threshold in thresholds:\n",
    "        segments = apply_silence_threshold(appended_segments, threshold)\n",
    "        \n",
    "        # Get all silence and speech segments durations\n",
    "        silence_durations = [segment['end'] - segment['start'] for segment in segments if not segment['is_speech']]\n",
    "        speech_durations = [segment['end'] - segment['start'] for segment in segments if segment['is_speech']]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        silence_avg = np.mean(silence_durations) if silence_durations else 0\n",
    "        speech_avg = np.mean(speech_durations) if speech_durations else 0\n",
    "        silence_min = np.min(silence_durations) if silence_durations else 0\n",
    "        silence_max = np.max(silence_durations) if silence_durations else 0\n",
    "        speech_min = np.min(speech_durations) if speech_durations else 0\n",
    "        speech_max = np.max(speech_durations) if speech_durations else 0\n",
    "        \n",
    "        # Append to lists\n",
    "        silence_avg_durations.append(silence_avg)\n",
    "        speech_avg_durations.append(speech_avg)\n",
    "        silence_min_durations.append(silence_min)\n",
    "        silence_max_durations.append(silence_max)\n",
    "        speech_min_durations.append(speech_min)\n",
    "        speech_max_durations.append(speech_max)\n",
    "    \n",
    "    # Convert to minutes if values are very large\n",
    "    convert_to_minutes = max(max(silence_max_durations), max(speech_max_durations)) > 1000\n",
    "    if convert_to_minutes:\n",
    "        silence_avg_durations = [d / 60 for d in silence_avg_durations]\n",
    "        speech_avg_durations = [d / 60 for d in speech_avg_durations]\n",
    "        silence_min_durations = [d / 60 for d in silence_min_durations]\n",
    "        silence_max_durations = [d / 60 for d in silence_max_durations]\n",
    "        speech_min_durations = [d / 60 for d in speech_min_durations]\n",
    "        speech_max_durations = [d / 60 for d in speech_max_durations]\n",
    "        y_label = 'Duration (minutes)'\n",
    "    else:\n",
    "        y_label = 'Duration (seconds)'\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Width of each bar\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    # Set positions for grouped bars\n",
    "    positions = np.arange(len(thresholds))\n",
    "    silence_positions = positions - bar_width/2\n",
    "    speech_positions = positions + bar_width/2\n",
    "    \n",
    "    # Calculate error bar ranges (distance from average to min/max)\n",
    "    silence_min_error = [avg - min_val for avg, min_val in zip(silence_avg_durations, silence_min_durations)]\n",
    "    silence_max_error = [max_val - avg for avg, max_val in zip(silence_avg_durations, silence_max_durations)]\n",
    "    speech_min_error = [avg - min_val for avg, min_val in zip(speech_avg_durations, speech_min_durations)]\n",
    "    speech_max_error = [max_val - avg for avg, max_val in zip(speech_avg_durations, speech_max_durations)]\n",
    "    \n",
    "    # Create error bars as [lower error, upper error]\n",
    "    silence_error = [silence_min_error, silence_max_error]\n",
    "    speech_error = [speech_min_error, speech_max_error]\n",
    "    \n",
    "    # Create bars\n",
    "    silence_bars = ax.bar(silence_positions, silence_avg_durations, bar_width, \n",
    "                         label='Silence', color='lightgray', yerr=silence_error, \n",
    "                         capsize=5, alpha=0.8)\n",
    "    speech_bars = ax.bar(speech_positions, speech_avg_durations, bar_width,\n",
    "                         label='Speech', color='steelblue', yerr=speech_error, \n",
    "                         capsize=5, alpha=0.8)\n",
    "    \n",
    "    # Add labels and styling\n",
    "    ax.set_xlabel('Silence Threshold')\n",
    "    ax.set_ylabel(f'Average {y_label}')\n",
    "    ax.set_title('Average Duration of Speech and Silence Segments by Threshold\\n(Whiskers show min/max values)')\n",
    "    ax.set_xticks(positions)\n",
    "    ax.set_xticklabels(thresholds)\n",
    "    ax.legend()\n",
    "\n",
    "    # Add y axis grid\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    # Add markers for every 2 minutes if values are in minutes\n",
    "    if y_label == 'Duration (minutes)':\n",
    "        ax.yaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "\n",
    "    # Add segment counts as text on each bar\n",
    "    for threshold_idx, threshold in enumerate(thresholds):\n",
    "        segments = apply_silence_threshold(appended_segments, threshold)\n",
    "        silence_count = sum(1 for segment in segments if not segment['is_speech'])\n",
    "        speech_count = sum(1 for segment in segments if segment['is_speech'])\n",
    "        \n",
    "        # Add count labels\n",
    "        ax.text(silence_positions[threshold_idx], silence_avg_durations[threshold_idx]/2,\n",
    "                f\"n={silence_count}\", ha='center', va='center', color='black', fontsize=8)\n",
    "        ax.text(speech_positions[threshold_idx], speech_avg_durations[threshold_idx]/2,\n",
    "                f\"n={speech_count}\", ha='center', va='center', color='white', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example thresholds to test\n",
    "thresholds = [0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4]\n",
    "\n",
    "# Run the function\n",
    "plot_speech_silence_distribution(appended_segments, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways:\n",
    "- A silence threshold of 1 second leads to all snippets being below 2min and on average around 15 to 20s. Seems interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-coco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
