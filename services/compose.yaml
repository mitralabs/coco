services:

  chunking:
    build:
      context: .
      dockerfile: chunking/Dockerfile
    ports:
      - "8001:8000"
    volumes:
      - ./chunking/app:/app
      - ./_data:/data
    environment:
      - API_KEY=test
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "-X", "GET", "-H", "X-API-Key: test", "http://localhost:8000/test" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  db-api:
    build:
      context: .
      dockerfile: db_api/Dockerfile
    ports:
      - "8003:8000"
    volumes:
      - ./db_api/app:/app
      - ./_data:/data
    environment:
      - API_KEY=test
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=coco
      - POSTGRES_HOST=db
      - EMBEDDING_DIM=1024
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "-X", "GET", "-H", "X-API-Key: test", "http://localhost:8000/test" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    depends_on:
      db:
        condition: service_healthy

  db:
    image: pgvector/pgvector:0.8.0-pg17
    volumes:
      - ./_data/db:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=coco
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    ports:
      - "8002:7860"
    volumes:
      - ../python_sdk:/python_sdk
      - ./frontend/app:/app
      - ./_data/audio_files:/app/audio
    env_file:
      - ".env"
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - COCO_CHUNK_URL_BASE=http://chunking:8000
      - COCO_DB_API_URL_BASE=http://db-api:8000
      - COCO_TRANSCRIPTION_URL_BASE=http://host.docker.internal:8000
      - COCO_OLLAMA_URL_BASE=http://host.docker.internal:11434
      - COCO_OPENAI_URL_BASE=https://openai.inference.de-txl.ionos.com/v1
      - COCO_EMBEDDING_API=openai
      - COCO_LLM_API=openai
      - COCO_API_KEY=test
      - COCO_EMBEDDING_MODEL=BAAI/bge-m3
      - COCO_DEFAULT_LLM_MODEL=meta-llama/Llama-3.3-70B-Instruct
    restart: unless-stopped
    depends_on:
      chunking:
        condition: service_healthy
      db-api:
        condition: service_healthy

  orchestrator:
    build:
      context: .
      dockerfile: orchestrator/Dockerfile
    ports:
      - "3030:8000"
    volumes:
      - ../python_sdk:/python_sdk
      - ./orchestrator/app:/app
      - ./_data/audio_files:/data
    environment:
      - COCO_CHUNK_URL_BASE=http://chunking:8000
      - COCO_DB_API_URL_BASE=http://db-api:8000
      - COCO_TRANSCRIPTION_URL_BASE=http://host.docker.internal:8000
      - COCO_OLLAMA_URL_BASE=http://host.docker.internal:11434
      - COCO_OPENAI_URL_BASE=https://openai.inference.de-txl.ionos.com/v1
      - COCO_EMBEDDING_API=openai
      - COCO_LLM_API=openai
      - COCO_API_KEY=test
      - COCO_EMBEDDING_MODEL=BAAI/bge-m3
    env_file:
      - ".env"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f -X GET -H 'X-API-Key: test' http://localhost:8000/test && curl -f -X GET -H 'X-API-Key: test' http://host.docker.internal:8000/test"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    depends_on:
      chunking:
        condition: service_healthy
      db-api:
        condition: service_healthy

  mcp_server:
    container_name: coco_mcp_server
    build:
      context: .
      dockerfile: mcp_server/Dockerfile
    stdin_open: true  # This is equivalent to docker run's -i flag
    volumes:
      - ../python_sdk:/python_sdk
      - ./mcp_server/app:/app
      - ./_data/mcp_logs:/logs
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/coco
      - COCO_EMBEDDING_API=ollama
      - COCO_LLM_API=ollama
      - COCO_OLLAMA_URL_BASE=http://host.docker.internal:11434
      - COCO_OPENAI_URL_BASE=https://openai.inference.de-txl.ionos.com/v1
      - COCO_EMBEDDING_MODEL=bge-m3
      - COCO_API_KEY=test
    env_file:
      - ".env"
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy