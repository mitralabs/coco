# This API_KEY is needed for each container to communicate with each other. Change this if you plan on using this on remote servers. -> Be sure to change this within the compose.yaml file as well !!
API_KEY=local

# This is the model used for the Whisper API. Make sure to use an available model (see readme)
WHISPER_MODEL=large-v3-turbo

# These variables are needed for the database services. No need to change them unless you want to.
POSTGRES_DB=coco
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_HOST=db


# If you plan on using Ollama leave this as is. If you plan on using an OpenAI like Endpoint, comment this out and uncomment the OpenAI variables below.
COCO_OLLAMA_URL_BASE=http://host.docker.internal:11434
COCO_EMBEDDING_API=ollama
COCO_LLM_API=ollama
# Make sure to use the correct model name. You can find a list of your downloaded models by running `ollama list` in your terminal.
COCO_EMBEDDING_MODEL=bge-m3
# The embedding dimension of the embedding model you are using. It is 764 for the nomic embedding model and 1024 for bge-m3. In theory it should work, if you set it to a value greater than the actual embedding dimension, but it is not recommended.
EMBEDDING_DIM=1024

## If you plan on using OpenAI, uncomment this and comment the Ollama variables above.
# COCO_OPENAI_URL_BASE=https://openai.inference.de-txl.ionos.com/v1
# COCO_EMBEDDING_API=openai
# COCO_LLM_API=openai
# OPENAI_API_KEY = your_api_key_if_you_want_to_use_openai
## Make sure to use the correct model name.
# COCO_EMBEDDING_MODEL=BAAI/bge-m3
## The embedding dimension of the embedding model you are using. It is 764 for the nomic embedding model and 1024 for bge-m3. In theory it should work, if you set it to a value greater than the actual embedding dimension, but it is not recommended.
# EMBEDDING_DIM=1024